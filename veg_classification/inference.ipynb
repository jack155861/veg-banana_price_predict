{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584a033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "def swish(x, beta = 1):\n",
    "    # https://www.geeksforgeeks.org/ml-swish-function-by-google-in-keras/\n",
    "    return (x * sigmoid(beta * x))\n",
    "def leaky_relu(x):\n",
    "    return tf.nn.leaky_relu(x, alpha=0.25)\n",
    "def get_weighted_loss(weights):\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "    return weighted_loss\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "def image_convert(a):\n",
    "    new = np.zeros((30, 30))\n",
    "    for i in range(30):\n",
    "        if int(a[i]*100)==0:\n",
    "            d = 29\n",
    "        elif int(a[i]*100)==100:\n",
    "            d = 0\n",
    "        else:\n",
    "            d = 30-int(30-(1-a[i])*30)\n",
    "            if d==30:\n",
    "                d = 29\n",
    "        new[d, i] = 1\n",
    "    return new\n",
    "def parsing_groupby(data_train_, ind):\n",
    "    df = data_train_.groupby([ind,ind+'-P']).size().rename('num').reset_index(drop = False)\n",
    "    df.columns = ['before', 'after', 'num']\n",
    "    df['col'] = ind\n",
    "    return df\n",
    "\n",
    "get_custom_objects().update({'swish': swish})\n",
    "get_custom_objects().update({'leaky_relu': leaky_relu})\n",
    "get_custom_objects().update({\"get_weighted_loss\": get_weighted_loss})\n",
    "get_custom_objects().update({\"weighted_loss\": weighted_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8775dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "banana_X_train = pd.read_pickle(\"veg_X_train.pkl\")\n",
    "banana_X_val = pd.read_pickle(\"veg_X_val.pkl\")\n",
    "banana_Y_train = pd.read_pickle(\"veg_Y_train.pkl\")\n",
    "banana_Y_val = pd.read_pickle(\"veg_Y_val.pkl\")\n",
    "cols_name = [[y+'-'+str(x).rjust(2,\"0\") for x in range(1,31)] for y in ['RSV','上價中位數','下價中位數','中價中位數','交易量','平均價','雨量']]\n",
    "cols_name_2 = [x for x in banana_X_train.columns.tolist() if '-' not in x]\n",
    "\n",
    "train_X_num = banana_X_train[cols_name_2[1:]].values\n",
    "val_X_num = banana_X_val[cols_name_2[1:]].values\n",
    "\n",
    "train_Y_1 = banana_Y_train[[x for x in banana_Y_train.columns if '成本價格' in x]].values\n",
    "val_Y_1 = banana_Y_val[[x for x in banana_Y_val.columns if '成本價格' in x]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "659eb499",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_lstm_1 = []\n",
    "for i in range(len(cols_name)):\n",
    "    tmp = banana_X_train[cols_name[i]].iloc[:,-7:]\n",
    "    train_X_lstm_1.append(tmp)\n",
    "train_X_lstm_1 = np.stack(train_X_lstm_1,axis=2)\n",
    "val_X_lstm_1 = []\n",
    "for i in range(len(cols_name)):\n",
    "    tmp = banana_X_val[cols_name[i]].iloc[:,-7:]\n",
    "    val_X_lstm_1.append(tmp)\n",
    "val_X_lstm_1 = np.stack(val_X_lstm_1,axis=2)\n",
    "\n",
    "train_X_lstm_2 = []\n",
    "for i in range(len(cols_name)):\n",
    "    tmp_all = []\n",
    "    tmp = banana_X_train[cols_name[i]].iloc[:,-13:]\n",
    "    for j in range(7):\n",
    "        tmp_1 = tmp.iloc[:,j:j+7]\n",
    "        tmp_all.append(tmp_1)\n",
    "    tmp_all = np.stack(tmp_all,axis=2)\n",
    "    train_X_lstm_2.append(tmp_all)\n",
    "train_X_lstm_2 = np.stack(train_X_lstm_2,axis=3)\n",
    "\n",
    "val_X_lstm_2 = []\n",
    "for i in range(len(cols_name)):\n",
    "    tmp_all = []\n",
    "    tmp = banana_X_val[cols_name[i]].iloc[:,-13:]\n",
    "    for j in range(7):\n",
    "        tmp_1 = tmp.iloc[:,j:j+7]\n",
    "        tmp_all.append(tmp_1)\n",
    "    tmp_all = np.stack(tmp_all,axis=2)\n",
    "    val_X_lstm_2.append(tmp_all)\n",
    "val_X_lstm_2 = np.stack(val_X_lstm_2,axis=3)\n",
    "\n",
    "train_X_lstm_4 = np.stack([np.stack([image_convert(x) for x in banana_X_train[cols_name[y]].values]) for y in range(len(cols_name))], axis=3)\n",
    "val_X_lstm_4 = np.stack([np.stack([image_convert(x) for x in banana_X_val[cols_name[y]].values]) for y in range(len(cols_name))], axis=3)\n",
    "\n",
    "data_X = {'Conv1D':[train_X_lstm_1, val_X_lstm_1],\n",
    "          'Vanilla':[train_X_lstm_1, val_X_lstm_1],\n",
    "          'Stacked':[train_X_lstm_1, val_X_lstm_1],\n",
    "          'Bidirectional':[train_X_lstm_1, val_X_lstm_1],\n",
    "          'Conv1D_LSTM':[train_X_lstm_2, val_X_lstm_2],\n",
    "          'Conv2D_1':[train_X_lstm_2, val_X_lstm_2],\n",
    "          'Conv2D_2':[train_X_lstm_4, val_X_lstm_4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52861af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(source, x1, model_, x2, data_all):\n",
    "    data_train = model_.predict([x1, x2], verbose = 0)\n",
    "    data_train[data_train>=0.5] = 1\n",
    "    data_train[data_train<0.5] = 0\n",
    "    data_train = pd.DataFrame(data_train)\n",
    "    data_train.columns = ['成本價格-02-P', '成本價格-03-P', '成本價格-04-P', '成本價格-05-P', '成本價格-06-P']\n",
    "    data_train = pd.concat([data_all.reset_index(drop = True), data_train], axis=1)\n",
    "    data_train['source'] = source\n",
    "\n",
    "    df = pd.concat([parsing_groupby(data_train, x) for x in ['成本價格-02','成本價格-03','成本價格-04','成本價格-05','成本價格-06']])\n",
    "    df = pd.merge(df,\n",
    "                  df.groupby(['col','before'])['num'].sum().reset_index(drop = False).rename(columns={'num':'num_total'}),\n",
    "                  on = ['col','before'],\n",
    "                  how = 'left')\n",
    "    df['per'] = df['num']/df['num_total']\n",
    "    df['source'] = source\n",
    "    return data_train, df\n",
    "def predict_all(list_route_, dataset, data_X_ = data_X):\n",
    "    if dataset == \"train\":\n",
    "        X = data_X_[list_route_.split(\"/\")[1]][0]\n",
    "        Y = train_X_num\n",
    "        al = banana_Y_train\n",
    "    else:\n",
    "        X = data_X_[list_route_.split(\"/\")[1]][1]\n",
    "        Y = val_X_num\n",
    "        al = banana_Y_val\n",
    "        \n",
    "    final_model_ = list_route_.replace(\"history.png\", \"final.h5\")\n",
    "    final_model = load_model(final_model_)\n",
    "    data_all1, data_groupby1 = model_predict(source = final_model_, \n",
    "                                             x1 = X, x2 = Y, data_all = al,\n",
    "                                             model_ = final_model)\n",
    "\n",
    "    acc_model_ = list_route_.replace(\"history.png\", \"weights_accuracy.hdf5\")\n",
    "    acc_model = load_model(acc_model_)\n",
    "    data_all2, data_groupby2 = model_predict(source = acc_model_, \n",
    "                                             x1 = X, x2 = Y, data_all = al,\n",
    "                                             model_ = final_model)\n",
    "\n",
    "    loss_model_ = list_route_.replace(\"history.png\", \"weights_loss.hdf5\")\n",
    "    loss_model = load_model(loss_model_)\n",
    "    data_all3, data_groupby3 = model_predict(source = loss_model_, \n",
    "                                             x1 = X, x2 = Y, data_all = al,\n",
    "                                             model_ = final_model)\n",
    "\n",
    "    data_all = pd.concat([data_all1,data_all2,data_all3]).reset_index(drop = True)\n",
    "    data_groupby = pd.concat([data_groupby1,data_groupby2,data_groupby3]).reset_index(drop = True)\n",
    "    data_all['dataset'] = dataset\n",
    "    data_groupby['dataset'] = dataset\n",
    "    return data_all, data_groupby\n",
    "def cal_all_acc(list_route_):\n",
    "    data_all = []\n",
    "    data_groupby = []\n",
    "    data_error = []\n",
    "    for i in range(len(list_route_)):\n",
    "        print(i, len(list_route_))\n",
    "        try:\n",
    "            data_all_train, data_groupby_train = predict_all(list_route_[i], dataset = 'train')\n",
    "            data_all_val, data_groupby_val = predict_all(list_route_[i], dataset = 'val')\n",
    "            result = \"OK\"\n",
    "        except:\n",
    "            print(\"error\", list_route_[i])\n",
    "            data_error.append(list_route_[i])\n",
    "            result = \"fail\"\n",
    "        if result == \"OK\":\n",
    "            data_all_ = pd.concat([data_all_train, data_all_val]).reset_index(drop = True)\n",
    "            data_groupby_ = pd.concat([data_groupby_train, data_groupby_val]).reset_index(drop = True)\n",
    "            data_all.append(data_all_)\n",
    "            data_groupby.append(data_groupby_)\n",
    "            del data_all_train, data_groupby_train, data_all_val, data_groupby_val, result, data_all_, data_groupby_\n",
    "    data_all = pd.concat(data_all).reset_index(drop = True)\n",
    "    data_groupby = pd.concat(data_groupby).reset_index(drop = True)\n",
    "    return data_all, data_groupby, data_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066da2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e9fac",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 125\n",
      "0 52\n",
      "error model/Conv2D_1/0.1-adam-leaky_relu-gelu/history.png\n",
      "error model/Bidirectional/0.01-nadam-tanh-relu/history.png\n",
      "1 125\n",
      "1 125\n",
      "1 125\n",
      "1 125\n",
      "1 125\n",
      "1 125\n",
      "1 125\n",
      "1 125\n",
      "1 125\n",
      "1 125\n",
      "1 125\n",
      "1 52\n",
      "1 125\n",
      "1 125\n",
      "1 125\n",
      "1 125\n",
      "1 125\n",
      "2 125\n",
      "2 125\n",
      "2 125\n",
      "2 125\n",
      "2 125\n",
      "2 125\n",
      "2 125\n",
      "2 125\n",
      "2 52\n",
      "2 125\n",
      "2 125\n",
      "2 125\n",
      "2 125\n",
      "3 125\n",
      "3 125\n",
      "2 125\n",
      "2 125\n",
      "3 125\n",
      "2 125\n",
      "3 125\n",
      "error model/Conv2D_1/0-nadam-relu-relu/history.png\n",
      "error model/Bidirectional/0.0001-adam-gelu-leaky_relu/history.png\n",
      "3 52\n",
      "3 125\n",
      "3 125\n",
      "2 125\n",
      "3 125\n",
      "3 125\n",
      "3 125\n",
      "4 125\n",
      "4 125\n",
      "4 125\n",
      "3 125\n",
      "3 125\n",
      "4 125\n",
      "3 125\n",
      "4 125\n",
      "3 125\n",
      "4 52\n",
      "4 125\n",
      "3 125\n",
      "3 125\n",
      "3 125\n",
      "5 125\n",
      "4 125\n",
      "4 125\n",
      "5 125\n",
      "5 125\n",
      "4 125\n",
      "5 125\n",
      "4 125\n",
      "4 125\n",
      "4 125\n",
      "5 125\n",
      "4 125\n",
      "6 125\n",
      "5 52\n",
      "5 125\n",
      "5 125\n",
      "5 125\n",
      "4 125\n",
      "6 125\n",
      "4 125\n",
      "6 125\n",
      "4 125\n",
      "error model/Conv1D/0.1-adam-swish-gelu/history.png\n",
      "5 125\n",
      "error model/Bidirectional/0.01-sgd-gelu-gelu/history.png\n",
      "6 125\n",
      "7 125\n",
      "5 125\n",
      "5 125\n",
      "6 125\n",
      "5 125\n",
      "7 125\n",
      "6 52\n",
      "7 125\n",
      "5 125\n",
      "6 125\n",
      "6 125\n",
      "8 125\n",
      "6 125\n",
      "7 125\n",
      "5 125\n",
      "5 125\n",
      "6 125\n",
      "6 125\n",
      "7 125\n",
      "5 125\n",
      "8 125\n",
      "6 125\n",
      "8 125\n",
      "6 125\n",
      "7 52\n",
      "7 125\n",
      "7 125\n",
      "8 125\n",
      "9 125\n",
      "6 125\n",
      "7 125\n",
      "9 125\n",
      "8 125\n",
      "9 125\n",
      "6 125\n",
      "7 125\n",
      "7 125\n",
      "7 125\n",
      "6 125\n",
      "8 125\n",
      "8 125\n",
      "7 125\n",
      "6 125\n",
      "9 125\n",
      "10 125\n",
      "8 52\n",
      "7 125\n",
      "10 125\n",
      "10 125\n",
      "9 125\n",
      "error model/Bidirectional/0.001-nadam-tanh-gelu/history.png\n",
      "error model/Bidirectional/0-nadam-swish-relu/history.png\n",
      "10 125\n",
      "8 125\n",
      "11 125\n",
      "8 125\n",
      "8 125\n",
      "8 125\n",
      "9 125\n",
      "7 125\n",
      "10 125\n",
      "9 125\n",
      "error model/Conv2D_1/0.0001-nadam-tanh-relu/history.png\n",
      "error model/Conv1D/0.001-sgd-relu-leaky_relu/history.png\n",
      "8 125\n",
      "9 125\n",
      "7 125\n",
      "8 125\n",
      "7 125\n",
      "11 125\n",
      "11 125\n",
      "9 52\n",
      "8 125\n",
      "11 125\n",
      "10 125\n",
      "12 125\n",
      "11 125\n",
      "10 125\n",
      "9 125\n",
      "9 125\n",
      "9 125\n",
      "12 125\n",
      "9 125\n",
      "12 125\n",
      "12 125\n",
      "9 125\n",
      "10 125\n",
      "11 125\n",
      "8 125\n",
      "10 52\n",
      "12 125\n",
      "13 125\n",
      "8 125\n",
      "9 125\n",
      "13 125\n",
      "11 125\n",
      "13 125\n",
      "10 125\n",
      "10 125\n",
      "10 125\n",
      "13 125\n",
      "10 125\n",
      "11 125\n",
      "12 125\n",
      "13 125\n",
      "10 125\n",
      "14 125\n",
      "14 125\n",
      "11 52\n",
      "14 125\n",
      "9 125\n",
      "10 125\n",
      "12 125\n",
      "9 125\n",
      "15 125\n",
      "14 125\n",
      "13 125\n",
      "11 125\n",
      "11 125\n",
      "14 125\n",
      "11 125\n",
      "11 125\n",
      "12 125\n",
      "15 125\n",
      "15 125\n",
      "11 125\n",
      "13 125\n",
      "16 125\n",
      "12 52\n",
      "14 125\n",
      "11 125\n",
      "15 125\n",
      "10 125\n",
      "15 125\n",
      "12 125\n",
      "10 125\n",
      "12 125\n",
      "12 125\n",
      "16 125\n",
      "16 125\n",
      "17 125\n",
      "12 125\n",
      "12 125\n",
      "13 125\n",
      "16 125\n",
      "14 125\n",
      "16 125\n",
      "15 125\n",
      "13 52\n",
      "13 125\n",
      "12 125\n",
      "17 125\n",
      "13 125\n",
      "11 125\n",
      "13 125\n",
      "17 125\n",
      "18 125\n",
      "11 125\n",
      "17 125\n",
      "13 125\n",
      "17 125\n",
      "15 125\n",
      "14 125\n",
      "13 125\n",
      "16 125\n",
      "19 125\n",
      "18 125\n",
      "13 125\n",
      "14 52\n",
      "14 125\n",
      "18 125\n",
      "18 125\n",
      "14 125\n",
      "14 125\n",
      "12 125\n",
      "18 125\n",
      "12 125\n",
      "17 125\n",
      "20 125\n",
      "15 125\n",
      "14 125\n",
      "16 125\n",
      "19 125\n",
      "14 125\n",
      "19 125\n",
      "error model/Bidirectional/0.001-nadam-leaky_relu-leaky_relu/history.png\n",
      "19 125\n",
      "19 125\n",
      "14 125\n",
      "15 125\n",
      "15 52\n",
      "15 125\n",
      "21 125\n",
      "15 125\n",
      "18 125\n",
      "17 125\n",
      "20 125\n",
      "16 125\n",
      "13 125\n",
      "13 125\n",
      "error model/Vanilla/0.1-sgd-gelu-leaky_relu/history.png\n",
      "22 125\n",
      "error model/Stacked/0.001-nadam-leaky_relu-swish/history.png\n",
      "18 125\n",
      "20 125\n",
      "20 125\n",
      "15 125\n",
      "15 125\n",
      "20 125\n",
      "15 125\n",
      "21 125\n",
      "16 125\n",
      "16 52\n"
     ]
    }
   ],
   "source": [
    "list_route = [os.path.join(path, name) for path, subdirs, files in os.walk('model') for name in files]\n",
    "list_route = [x for x in list_route if 'history.png' in x]\n",
    "list_route = [list_route[x:x+125] for x in range(0, len(list_route), 125)]\n",
    "print(len(list_route))\n",
    "#list_route = [x for x in list_route if 'history.png' in x][50:100]\n",
    "#cal_all_acc(list_route)\n",
    "pool = ThreadPool(len(list_route))\n",
    "data = pool.map(cal_all_acc, list_route)\n",
    "\n",
    "data_all = pd.concat([x[0] for x in data]).reset_index(drop = True)\n",
    "data_groupby = pd.concat([x[1] for x in data]).reset_index(drop = True)\n",
    "error_list = sum([x[2] for x in data], [])\n",
    "data_all.to_csv(\"inference_all_veg_classification.csv\", index = False)\n",
    "data_groupby.to_csv(\"inference_groupby_veg_classification.csv\", index = False)\n",
    "#data_error = cal_all_acc(error_list)\n",
    "#data_error[0].to_csv(\"inference_all_veg_classification_2.csv\", index = False)\n",
    "#data_error[1].to_csv(\"inference_groupby_veg_classification_2.csv\", index = False)\n",
    "#data_error[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae804136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/Conv1D_LSTM/0.001-sgd-gelu-leaky_relu/history.png',\n",
       " 'model/Bidirectional/0.01-nadam-gelu-relu/history.png',\n",
       " 'model/Stacked/0.0001-sgd-relu-gelu/history.png']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_route = [os.path.join(path, name) for path, subdirs, files in os.walk('model') for name in files]\n",
    "list_route = [x for x in list_route if 'history.png' in x]\n",
    "exist_file = list(set(['/'.join(x.split(\"/\")[:-1])+\"/history.png\" for x in set(pd.unique(pd.read_csv(\"inference_groupby_veg_classification.csv\")['source']))]))\n",
    "list_route = list(set(list_route)-set(exist_file))\n",
    "len(list_route)\n",
    "list_route = [list_route[x:x+30] for x in range(0, len(list_route), 30)]\n",
    "print(len(list_route))\n",
    "pool = ThreadPool(len(list_route))\n",
    "data = pool.map(cal_all_acc, list_route)\n",
    "\n",
    "data_all = pd.concat([x[0] for x in data]).reset_index(drop = True)\n",
    "data_groupby = pd.concat([x[1] for x in data]).reset_index(drop = True)\n",
    "error_list = sum([x[2] for x in data], [])\n",
    "data_all.to_csv(\"inference_all_veg_classification_2.csv\", index = False)\n",
    "data_groupby.to_csv(\"inference_groupby_veg_classification_2.csv\", index = False)\n",
    "error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5ae5aab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3\n",
      "1 3\n",
      "2 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_error = cal_all_acc(error_list)\n",
    "data_error[0].to_csv(\"inference_all_veg_classification_3.csv\", index = False)\n",
    "data_error[1].to_csv(\"inference_groupby_veg_classification_3.csv\", index = False)\n",
    "data_error[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381fe4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a1cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_groupby = pd.concat([pd.read_csv(\"inference_groupby_veg_classification.csv\"),\n",
    "                          pd.read_csv(\"inference_groupby_veg_classification_2.csv\"),\n",
    "                          pd.read_csv(\"inference_groupby_veg_classification_3.csv\")]).reset_index(drop = True)\n",
    "data_groupby.to_csv(\"tmp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a6d6fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "per = 0.965\n",
    "a = data_groupby[data_groupby['before']==data_groupby['after']]\n",
    "a = a.pivot(index=['source','dataset'],columns=['before','col'],values='per').reset_index(drop = False)\n",
    "a.columns = [str(x[0])+str(x[1]) for x in a.columns.tolist()]\n",
    "a_list = a[(a['dataset']==\"val\")&(a['0.0成本價格-02']>=per)&(a['1.0成本價格-02']>=per)&(a['0.0成本價格-03']>=per)&(a['1.0成本價格-03']>=per)&(a['0.0成本價格-04']>=per)&(a['1.0成本價格-04']>=per)&(a['0.0成本價格-05']>=per)&(a['1.0成本價格-05']>=per)&(a['0.0成本價格-06']>=per)&(a['1.0成本價格-06']>=per)]['source'].tolist()\n",
    "b_list = a[(a['dataset']==\"train\")&(a['0.0成本價格-02']>=per)&(a['1.0成本價格-02']>=per)&(a['0.0成本價格-03']>=per)&(a['1.0成本價格-03']>=per)&(a['0.0成本價格-04']>=per)&(a['1.0成本價格-04']>=per)&(a['0.0成本價格-05']>=per)&(a['1.0成本價格-05']>=per)&(a['0.0成本價格-06']>=per)&(a['1.0成本價格-06']>=per)]['source'].tolist()\n",
    "final_list = list(set(a_list)&set(b_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b78693f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>dataset</th>\n",
       "      <th>0.0成本價格-02</th>\n",
       "      <th>1.0成本價格-02</th>\n",
       "      <th>0.0成本價格-03</th>\n",
       "      <th>1.0成本價格-03</th>\n",
       "      <th>0.0成本價格-04</th>\n",
       "      <th>1.0成本價格-04</th>\n",
       "      <th>0.0成本價格-05</th>\n",
       "      <th>1.0成本價格-05</th>\n",
       "      <th>0.0成本價格-06</th>\n",
       "      <th>1.0成本價格-06</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model/Conv1D_LSTM/0-sgd-relu-relu/weights_accu...</td>\n",
       "      <td>train</td>\n",
       "      <td>0.974641</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969032</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971791</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973803</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973543</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model/Conv1D_LSTM/0-sgd-relu-relu/weights_accu...</td>\n",
       "      <td>val</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.980456</td>\n",
       "      <td>0.986486</td>\n",
       "      <td>0.969404</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977310</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.978862</td>\n",
       "      <td>0.972603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source dataset  0.0成本價格-02  \\\n",
       "0  model/Conv1D_LSTM/0-sgd-relu-relu/weights_accu...   train    0.974641   \n",
       "1  model/Conv1D_LSTM/0-sgd-relu-relu/weights_accu...     val    0.977273   \n",
       "\n",
       "   1.0成本價格-02  0.0成本價格-03  1.0成本價格-03  0.0成本價格-04  1.0成本價格-04  0.0成本價格-05  \\\n",
       "0    1.000000    0.969032    1.000000    0.971791         1.0    0.973803   \n",
       "1    0.986111    0.980456    0.986486    0.969404         1.0    0.977310   \n",
       "\n",
       "   1.0成本價格-05  0.0成本價格-06  1.0成本價格-06  \n",
       "0    1.000000    0.973543    1.000000  \n",
       "1    0.985915    0.978862    0.972603  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[(a['source'].isin(final_list))&(a['source'].str.contains('weights_accuracy.hdf5'))].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47acb5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model/Conv1D_LSTM/0-sgd-relu-relu'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['/'.join(x.split(\"/\")[:-1]) for x in final_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37de020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_route = [os.path.join(path, name) for path, subdirs, files in os.walk('model') for name in files]\n",
    "list_route = [x for x in list_route if 'history.png' in x]\n",
    "for i in range(len(list_route)):\n",
    "    if len(os.listdir('/'.join(list_route[i].split(\"/\")[:-1])))==7:\n",
    "        print(i, len(list_route))\n",
    "        _, data_groupby_val = predict_all(list_route[i], dataset = 'val')\n",
    "        a = data_groupby_val[data_groupby_val['before']==data_groupby_val['after']].reset_index(drop = True)\n",
    "        a['h5'] = [x.split(\"/\")[-1] for x in a['source'].tolist()]\n",
    "        a = a.pivot(index=['before','col'],columns='h5',values='per').reset_index(drop = False)\n",
    "        a = a[(a['final.h5']==a['weights_accuracy.hdf5'])&(a['final.h5']==a['weights_loss.hdf5'])].reset_index(drop = True)\n",
    "        if len(a)==10:\n",
    "            print(\"remove\", i)\n",
    "            if os.path.isfile(list_route[i].replace(\"history.png\", \"weights_accuracy.hdf5\")):\n",
    "                os.remove(list_route[i].replace(\"history.png\", \"weights_accuracy.hdf5\"))\n",
    "            if os.path.isfile(list_route[i].replace(\"history.png\", \"weights_loss.hdf5\")):\n",
    "                os.remove(list_route[i].replace(\"history.png\", \"weights_loss.hdf5\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
